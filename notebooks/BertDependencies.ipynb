{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tree.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvardnlp/pytorch-struct/blob/master/notebooks/BertDependencies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dENII4iGN3S4",
        "colab_type": "code",
        "outputId": "7dd16b64-34ca-46c6-d404-c9107587077a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install -qqq torchtext\n",
        "!pip install -qqq pytorch-transformers\n",
        "!pip install -qqqU git+https://github.com/harvardnlp/pytorch-struct\n",
        "!git clone -q http://github.com/srush/temp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for torch-struct (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "fatal: destination path 'temp' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EghAJ7aJDl4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_struct import DependencyCRF\n",
        "import torch_struct.data\n",
        "import torchtext.data as data\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "from pytorch_transformers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l63Zy6ap95Ug",
        "colab_type": "text"
      },
      "source": [
        "Parse the conll dependency data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQoaK1lv-CXc",
        "colab_type": "text"
      },
      "source": [
        "TorchText batching setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsKQXHflOmDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class, tokenizer_class, pretrained_weights = BertModel, BertTokenizer, 'bert-large-cased'\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "def batch_num(nums):\n",
        "    lengths = torch.tensor([len(n) for n in nums]).long()\n",
        "    n = lengths.max()\n",
        "    out = torch.zeros(len(nums), n).long()\n",
        "    for b, n in enumerate(nums):\n",
        "        out[b, :len(n)] = torch.tensor(n)\n",
        "    return out, lengths\n",
        "HEAD = data.RawField(preprocessing= lambda x: [int(i) for i in x],\n",
        "                     postprocessing=batch_num)\n",
        "WORD = torch_struct.data.SubTokenizedField(tokenizer)\n",
        "HEAD.is_target = True\n",
        "train = torch_struct.data.ConllXDataset(\"temp/wsj.train.conllx\", (('word', WORD), ('head', HEAD)),\n",
        "                     filter_pred=lambda x: 5 < len(x.word[0]) < 40)\n",
        "train_iter = torch_struct.data.TokenBucket(train, 750)\n",
        "val = torch_struct.data.ConllXDataset(\"temp/wsj.dev.conllx\", (('word', WORD), ('head', HEAD)),\n",
        "                     filter_pred=lambda x: 5 < len(x.word[0]) < 40)\n",
        "val_iter = torchtext.data.BucketIterator(val, \n",
        "    batch_size=20,\n",
        "    device=\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-40o2UA-Ha7",
        "colab_type": "text"
      },
      "source": [
        "Make a Bert model to compute the potentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUKVqGUsSmYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H = 1024 #768\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, hidden):\n",
        "        super().__init__()\n",
        "        self.base_model = model_class.from_pretrained(pretrained_weights)\n",
        "        self.linear = nn.Linear(H, H)\n",
        "        self.bilinear = nn.Linear(H, H)\n",
        "        self.root = nn.Parameter(torch.rand(H))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, words, mapper):\n",
        "        out = self.dropout(self.base_model(words))\n",
        "        out = torch.einsum(\"bca,bch->bah\", mapper.float().cuda(), out)\n",
        "        final2 = torch.einsum(\"bnh,hg->bng\", out, self.linear.weight)\n",
        "        final = torch.einsum(\"bnh,hg,bmg->bnm\", out, self.bilinear.weight, final2)\n",
        "        root_score = torch.einsum(\"bnh,h->bn\", out, self.root)\n",
        "        final = final[:, 1:-1, 1:-1]\n",
        "        N = final.shape[1]\n",
        "        final[:, torch.arange(N), torch.arange(N)] += root_score[:, 1:-1]\n",
        "        return final\n",
        "\n",
        "model = Model(H)\n",
        "wandb.watch(model)\n",
        "model.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grxGRu2b-LdT",
        "colab_type": "text"
      },
      "source": [
        "Generic training loop. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HENkCYOKSwHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate():\n",
        "    incorrect_edges = 0\n",
        "    total_edges = 0\n",
        "    model.eval()\n",
        "    for i, ex in enumerate(val_iter):\n",
        "        words, mapper, _ = ex.word\n",
        "        label, lengths = ex.head\n",
        "        batch, _ = label.shape\n",
        "\n",
        "        final = model(words.cuda(), mapper)\n",
        "        for b in range(batch):\n",
        "            final[b, lengths[b]-1:, :] = 0\n",
        "            final[b, :, lengths[b]-1:] = 0\n",
        "        dist = DependencyCRF(final, lengths=lengths)\n",
        "        argmax = dist.argmax\n",
        "        gold = dist.struct.to_parts(label, lengths=lengths).type_as(argmax)\n",
        "        incorrect_edges += (out[:, :].cpu() - gold[:, :].cpu()).abs().sum() / 2.0\n",
        "        total_edges += gold.sum()\n",
        "\n",
        "    print(total_edges, incorrect_edges)   \n",
        "    model.train()\n",
        "\n",
        "def train(train_iter, val_iter, model):\n",
        "    opt = AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
        "    scheduler = WarmupLinearSchedule(opt, warmup_steps=20, t_total=2500)\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for i, ex in enumerate(train_iter):\n",
        "        opt.zero_grad()\n",
        "        words, mapper, _ = ex.word\n",
        "        label, lengths = ex.head\n",
        "        batch, _ = label.shape\n",
        "        \n",
        "        # Model\n",
        "        final = model(words.cuda(), mapper)\n",
        "        for b in range(batch):\n",
        "            final[b, lengths[b]-1:, :] = 0\n",
        "            final[b, :, lengths[b]-1:] = 0\n",
        "        \n",
        "        if not lengths.max() <= final.shape[1] + 1:\n",
        "            print(\"fail\")\n",
        "            continue\n",
        "        dist = DependencyCRF(final, lengths=lengths)\n",
        "\n",
        "        labels = dist.struct.to_parts(label, lengths=lengths).type_as(final)\n",
        "        log_prob = dist.log_prob(final, labels)\n",
        "\n",
        "        loss = log_prob.sum()\n",
        "        (-loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        opt.step()\n",
        "        scheduler.step()\n",
        "        losses.append(loss.detach())\n",
        "        if i % 50 == 1:            \n",
        "            print(-torch.tensor(losses).mean(), words.shape)\n",
        "            losses = []\n",
        "        if i % 600 == 500:\n",
        "            validate(val_iter)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-HW3z1VT2MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(train_iter, val_iter, model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}